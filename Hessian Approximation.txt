#==================================
# Generate mock data by simulation
#==================================
n_features=2
n_data=100
set.seed(666)
x1 = rnorm(n_data, 0, 1)     # some continuous variables 
x2 = rnorm(n_data, 0, 1)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias

X<-cbind(rep(1,n_data), x1, x2)  # combine all our parameters in a single matrix

pr = 1/(1+exp(-z))         # our sigmoid/probability function
y = rbinom(length(x1),1,pr)      # bernoulli response variable

model.1 <- glm(y~x1+x2, family="binomial")
theta_true <- coef(model.1)  # the true values of coefficients

theta_init<-rep(0,n_features+1)   # initialize theta


#=====================================
# The Loss Function to be Minimized
#=====================================
Loss<-function(x,y,theta){
 L= sum(x%*%theta+log(1+exp(-x%*%theta))-y*(x%*%theta)) 
 return(L)
}



#==========================================
# Compute the Gradient Numerically
#==========================================

grad_vec<-function(x,y,theta)
{ delx=0.0001
grad_vec=c()
for(i in 1:length(theta))
	{ t_old=theta
	t_new=theta
	t_old[i]=theta[i]-delx
	t_new[i]=theta[i]+delx
	grad_vec[i]=(Loss(x,y,t_new)-Loss(x,y,t_old))/(2*delx)
	}
return(grad_vec)  
}


#==========================================
# Compute the Hessian Numerically
#==========================================

hessian<-function(x,y,theta)
{
  delx=0.001
  hessian=matrix(1,nrow=length(theta),ncol=length(theta))  # initialize a matrix of 1s
  temp=c()
  for(i in 1:length(theta))
  { for(j in 1:length(theta))
  {
    if(i==j)
    { t_old=theta
    t_new=theta
    t_old[i]=theta[i]-delx
    t_new[i]=theta[i]+delx
    gt_old=grad_vec(x,y,t_old)
    gt_new=grad_vec(x,y,t_new)
    hessian[i,i]=(gt_new[i]-gt_old[i])/(2*delx)}
    else
    { t_old=theta
    t_new=theta
    t_old[i]=theta[i]-delx
    t_new[i]=theta[i]+delx
    gt_old=grad_vec(x,y,t_old)
    gt_new=grad_vec(x,y,t_new)
    hessian[i,j]=(gt_new[j]-gt_old[j])/(2*delx)}
    hessian[j,i]=hessian[i,j]
  }
  }
  return(hessian)
}


#==========================
# Newton's Method
#==========================

newton<-function(x,y,theta,epsilon,max_iter)
{
  n = length(theta)
  func_old = Loss(x,y,theta)
  n_iter=0   # number of iterations
  for (k in 0: max_iter)
  {
    n_iter=n_iter+1
    grad=grad_vec(x,y,theta)  # this is a row vector
    gradv = matrix(grad ,nrow = n,ncol = 1)  # turn it into a column vector
    H=hessian(x,y,theta)  # construct hessian matrix
    Hinv=solve(H)
    s_dir=-Hinv%*%gradv    # search direction as a column vector
    s_dir_row=c(s_dir)    # search direction as a row vector (also t(s_dir))
    func_new=Loss(x,y,theta)  # calculate to report in case alg terminates next
    norm_grad=norm(grad,"2") #   ''''
    if(norm_grad<epsilon){break}
    func_old = func_new
    theta=theta+s_dir_row
  }
  
  # for display purposes inessential else
  derivative = grad_vec(x,y,theta)
  gradient = -derivative
  norm_gradient = norm(gradient ,"2")
  xm=theta     # this is the required solution
  results<-list(xm,norm_gradient,n_iter)
  return(results)
}


#==========================
# Now Test
#==========================


max_iter=100
epsilon=0.0001


#===============================
# compare theta with theta_true
#===============================

theta_true

newton(X,y,theta_init,epsilon,max_iter)

